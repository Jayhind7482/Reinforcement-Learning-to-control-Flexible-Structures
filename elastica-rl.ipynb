{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "from scipy.integrate import solve_bvp\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn' , force = True)\n",
    "import os\n",
    "from stable_baselines3 import PPO ,A2C , DDPG ,TD3 , SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv , SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box, Dict\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import math\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elastica_env(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(Elastica_env, self).__init__()\n",
    "        self.action_space = Box(low=np.array([-0.2, -0.2], dtype=np.float32), high=np.array([0.1 , 0.2], dtype=np.float64))\n",
    "        self.observation_space = Box(low=np.float32(-50), high=np.float32(50), shape=(13,), dtype=np.float64)\n",
    "        self.target = Box(low=np.array([4.5, -1], dtype=np.float32), high=np.array([5.57, 1], dtype=np.float64))\n",
    "        self.num_timestep = 0\n",
    "        self.reward = 0\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.screen_width = 800.0\n",
    "        self.screen_height = 600.0\n",
    "        self.zoom_factor = 60.0\n",
    "        self.enable_render = False\n",
    "        self.h = -0.4\n",
    "        self.v = 0.15\n",
    "        self.screen = None  # Add this to manage the screen as a persistent object\n",
    "        self.clock = None    # Add this to manage the clock as a persistent object\n",
    "        self.initialized_render = False\n",
    "\n",
    "    # Add the seed() method\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return seed\n",
    "    def step(self, action):\n",
    "        self.num_timestep += 1\n",
    "        self.h += action[0]\n",
    "        self.v += action[1]\n",
    "        self.X, self.Y, self.theta_dash_0  ,self.theta_dash_l, self.theta_l , self.E = self.elastica(self.h, self.v)\n",
    "\n",
    "        new_observation = self.get_observation()\n",
    "        self.render(self.enable_render)\n",
    "        self.reward = self.score()\n",
    "        done = self.get_done()\n",
    "        truncation = self.get_truncation()\n",
    "        info = {}\n",
    "        return new_observation, self.reward, done, truncation, info\n",
    "\n",
    "    def elastica(self, h, v):\n",
    "        l = 6\n",
    "        s = np.linspace(0, l, 500)\n",
    "        def f(s, y):\n",
    "            return np.vstack((y[1], h * np.sin(y[0]) - v * np.cos(y[0])))\n",
    "        def bc(ya, yb):\n",
    "            return np.array([ya[0] - 0, yb[1] - 0])\n",
    "        y0 = np.zeros((2, s.size))\n",
    "        sol = solve_bvp(f, bc, s, y0)\n",
    "        theta = sol.sol(s)[0]\n",
    "        dtheta_ds = sol.sol(s)[1]\n",
    "        y1 = np.cos(theta)\n",
    "        y2 = np.sin(theta)\n",
    "        y3 = (dtheta_ds)**2\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for i in range(len(s)):\n",
    "            self.x.append(np.trapz(y1[:i+1], x=s[:i+1]))\n",
    "            self.y.append(np.trapz(y2[:i+1], x=s[:i+1]))\n",
    "\n",
    "        e = 0.5*(np.trapz(y3 , s))-v*(np.trapz(y2 , s)) + h*(np.trapz(1-y1 , s))\n",
    "\n",
    "        return self.x, self.y, dtheta_ds[0] ,dtheta_ds[-1] , theta[-1] , e \n",
    "\n",
    "    def get_observation(self):\n",
    "        self.x_tip = self.X[-1]\n",
    "        self.y_tip = self.Y[-1]\n",
    "        d = ((self.x_tip - self.x_target)**2 + (self.y_tip - self.y_target)**2)**0.5\n",
    "        return np.array([self.x_tip, self.y_tip, self.X[200], self.Y[200], self.X[400], self.Y[400], \n",
    "                         self.theta_l , self.theta_dash_0 , self.theta_dash_l ,self.E  ,\n",
    "                         self.x_target, self.y_target ,d] ,  dtype=np.float64)\n",
    "\n",
    "    def score(self):\n",
    "        d = ((self.x_tip - self.x_target)**2 + (self.y_tip - self.y_target)**2)**0.5\n",
    "        #d_score = -(d)**2\n",
    "        d_score = math.exp(-d)\n",
    "        total_score = d_score\n",
    "        return total_score\n",
    "\n",
    "    def get_done(self):\n",
    "        done = False\n",
    "        d = ((self.x_tip - self.x_target)**2 + (self.y_tip - self.y_target)**2)**0.5\n",
    "        if d < 0.002:\n",
    "            done = True\n",
    "        return done\n",
    "\n",
    "    def get_truncation(self):\n",
    "        truncation = False\n",
    "        if self.num_timestep > 19 :\n",
    "            truncation = True\n",
    "        return truncation\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        targ = self.target.sample()\n",
    "        self.x_target = targ[0]\n",
    "        self.y_target = targ[1]\n",
    "        if self.y_target<=0:\n",
    "            self.h = -0.4\n",
    "            self.v = 0.15\n",
    "        if self.y_target>0:\n",
    "            self.h = -0.4\n",
    "            self.v = -0.15\n",
    "\n",
    "        self.X, self.Y, self.theta_dash_0 ,self.theta_dash_l , self.theta_l , self.E  = self.elastica(self.h, self.v)\n",
    "        self.num_timestep = 0\n",
    "        self.reward = 0\n",
    "        observation = self.get_observation()\n",
    "        info = {}\n",
    "        return observation, info\n",
    "  \n",
    "    def render(self, enable_render):\n",
    "        if not enable_render:\n",
    "            return\n",
    "\n",
    "        if not self.initialized_render:\n",
    "            # Initialize pygame components only once\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((int(self.screen_width), int(self.screen_height)))\n",
    "            pygame.display.set_caption(\"Elastica\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "            self.initialized_render = True\n",
    "\n",
    "    # Clear screen for new frame\n",
    "        self.screen.fill((255, 255, 255))\n",
    "        \n",
    "        offset_x = (self.screen_width - 10 * self.zoom_factor) / 2\n",
    "        offset_y = (self.screen_height - 1.5 * self.zoom_factor) / 2\n",
    "        points = [(self.X[i], self.Y[i]) for i in range(len(self.X))]\n",
    "\n",
    "    # Draw the elastica and its components\n",
    "        pygame.draw.lines(self.screen,(0, 0, 0),False,[(point[0] * self.zoom_factor + offset_x, point[1] * self.zoom_factor + offset_y)\n",
    "                                                       for point in points],)\n",
    "        pygame.draw.line(self.screen,(255, 0, 0),((self.X[-1]) * self.zoom_factor + offset_x,(self.Y[-1]) * self.zoom_factor + offset_y,),\n",
    "            ((self.X[-1]) * self.zoom_factor + offset_x + 50 * np.sign(self.h),(self.Y[-1]) * self.zoom_factor + offset_y,),3,)\n",
    "        \n",
    "        pygame.draw.line(self.screen,(0, 255, 0),((self.X[-1]) * self.zoom_factor + offset_x,(self.Y[-1]) * self.zoom_factor + offset_y,),\n",
    "            ((self.X[-1]) * self.zoom_factor + offset_x,(self.Y[-1]) * self.zoom_factor + offset_y + 50 * np.sign(self.v),),3,)\n",
    "        \n",
    "        pygame.draw.line(self.screen,(0, 0, 0),((self.X[0]) * self.zoom_factor + offset_x,(self.Y[0]) * self.zoom_factor + offset_y,),\n",
    "               ((self.X[0]) * self.zoom_factor + offset_x,(self.Y[0]) * self.zoom_factor + offset_y + 25,),3,)\n",
    "        \n",
    "        pygame.draw.line(self.screen,(0, 0, 0),((self.X[0]) * self.zoom_factor + offset_x,(self.Y[0]) * self.zoom_factor + offset_y,),\n",
    "            ((self.X[0]) * self.zoom_factor + offset_x,(self.Y[0]) * self.zoom_factor + offset_y - 25,),3,)\n",
    "        \n",
    "        pygame.draw.circle(self.screen,(255, 0, 0),(int(self.x_target * self.zoom_factor + offset_x),int(self.y_target * self.zoom_factor + offset_y),),5,)\n",
    "\n",
    "        # Display text information\n",
    "        font = pygame.font.Font(None, 36)\n",
    "        score_text = font.render(f\"Timesteps: {self.num_timestep}\", True, (0, 0, 0))\n",
    "        self.screen.blit(score_text,(int(self.screen_width - score_text.get_width() - 30), 120),)\n",
    "        \n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.close()\n",
    "                return\n",
    "            \n",
    "            elif event.type == pygame.VIDEORESIZE:\n",
    "                print(\"Window resized to:\", event.size)\n",
    "                self.screen = pygame.display.set_mode(event.size, pygame.RESIZABLE)\n",
    "                self.render(self.enable_render)  # Redraw everything after resizing\n",
    "            \n",
    "            elif event.type == pygame.ACTIVEEVENT:\n",
    "                if event.state == 1:  # Input focus (keyboard/mouse focus)\n",
    "                    if event.gain == 0:\n",
    "                        print(\"Window lost input focus\")\n",
    "                    elif event.gain == 1:\n",
    "                        print(\"Window gained input focus\")\n",
    "                elif event.state == 4:  # Active state (minimized/restored)\n",
    "                    if event.gain == 0:\n",
    "                        print(\"Window minimized or deactivated\")\n",
    "                        # Optional: Pause the game or rendering\n",
    "                    elif event.gain == 1:\n",
    "                        print(\"Window restored or activated\")\n",
    "                        self.render(self.enable_render)  # Redraw everything after restoration\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(30)  # Limit frame rate to 30 FPS\n",
    "\n",
    "    def close(self):\n",
    "        if self.initialized_render:\n",
    "            pygame.quit()\n",
    "            self.initialized_render = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Elastica_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:4.825024292335784\n",
      "Episode:2 Score:4.087163133816204\n",
      "Episode:3 Score:6.587745018432867\n",
      "Episode:4 Score:6.415459938447593\n",
      "Episode:5 Score:3.4319545520862706\n",
      "Episode:6 Score:4.817397620026562\n",
      "Episode:7 Score:4.109335225720665\n",
      "Episode:8 Score:5.377665455719757\n",
      "Episode:9 Score:6.116440766251155\n",
      "Episode:10 Score:1.5439731305725963\n"
     ]
    }
   ],
   "source": [
    "env.enable_render = True\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state , info = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    truncation=False\n",
    "    while not (done or truncation):\n",
    "        #env.render(True )\n",
    "        action = env.action_space.sample()\n",
    "        #print(action)\n",
    "        n_state, reward, done , truncation ,info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.enable_render = False\n",
    "log_path = os.path.join('Training' , 'Logs')\n",
    "os.makedirs(os.path.dirname(log_path) , exist_ok = True)\n",
    "eval_env = Elastica_env()\n",
    "eval_env = Monitor(eval_env , log_path)\n",
    "#policy_kwargs = dict(net_arch=dict(pi=[256, 256 , 256], qf=[500, 400 , 400]) )  # by default tanh activation function\n",
    "#policy_kwargs = dict(activation_fn=th.nn.ReLU,net_arch=dict(pi=[32, 32], qf=[32, 32]))    # ReLu activation function\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate' , 1e-5 , 1e-3 ,log = True)\n",
    "    entropy_coef = trial.suggest_float ('entropy_coef' , 1e-4 , 1e-2  , log = True)\n",
    "    gamma = trial.suggest_float('gamma' ,0.9 ,0.9999 , log = True)\n",
    "    tau = trial.suggest_float('tau' , 0.005 , 0.05 , log = True)\n",
    "    batch_size = trial.suggest_categorical('batch_size' , [64 , 128 , 256 , 1000 , 2000 , 4000 , 8000 , 16000])\n",
    "    target_update_interval = trial.suggest_categorical('target_update_interval' , [1000 , 5000 , 10000])\n",
    "    gradient_steps = trial.suggest_categorical('gradient_steps' , [1 , 2, 4])\n",
    "    buffer_size = trial.suggest_categorical('buffer_size' , [100000 , 200000 , 500000 , 1000000 , 2000000])\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, verbose=0 , learning_rate = learning_rate ,\n",
    "                ent_coef = entropy_coef , gamma = gamma , tau = tau , batch_size = batch_size , \n",
    "                target_update_interval = target_update_interval , gradient_steps= gradient_steps ,\n",
    "                buffer_size = buffer_size )\n",
    "    \n",
    "    eval_callback = EvalCallback(eval_env ,  eval_freq = 1000 ,deterministic = True )\n",
    "    model.learn(total_timesteps = 10000 , callback = eval_callback)\n",
    "\n",
    "    mean_reward , _ = evaluate_policy(model , eval_env , n_eval_episodes = 10 ,deterministic=True)\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study (direction = 'maximize')\n",
    "study.optimize(objective , n_trials = 50 , n_jobs = 1)\n",
    "best_parameters = study.best_params\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_path = os.path.join('Training', 'Logs')\n",
    "#PPO_path = os.path.join('Training', 'Saved Models', 'DDPG')\n",
    "#print(PPO_path)\n",
    "#env = DummyVecEnv([lambda: env])\n",
    "# model = SAC( 'MlpPolicy', env, verbose=0 ,  learning_rate=best_parameters['learning_rate'],\n",
    "#             ent_coef=best_parameters['entropy_coef'], gamma=best_parameters['gamma'], tau=best_parameters['tau'], \n",
    "#            batch_size=best_parameters['batch_size'], target_update_interval = best_parameters['target_update_interval'],\n",
    "#            gradient_steps=best_parameters['gradient_steps'],buffer_size=best_parameters['buffer_size'])  \n",
    "model = SAC( 'MlpPolicy', env, verbose=0 ,  learning_rate=0.000985293296715492,\n",
    "            ent_coef=0.0030394039632778013, gamma=0.9007530746938588, tau=0.03547973047760216, \n",
    "           batch_size=1000, target_update_interval = 5000 ,\n",
    "           gradient_steps=1,buffer_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-20T19:58:39.255820Z",
     "iopub.status.busy": "2024-11-20T19:58:39.255495Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=11.82 +/- 2.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=12.91 +/- 1.86\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=15.17 +/- 0.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=17.49 +/- 1.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=17.84 +/- 1.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=16.39 +/- 1.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=18.59 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=18.77 +/- 0.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=18.78 +/- 0.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=19.02 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=18.63 +/- 0.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=15.54 +/- 6.35\n",
      "Episode length: 16.60 +/- 6.80\n",
      "Eval num_timesteps=13000, episode_reward=19.46 +/- 0.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=19.24 +/- 0.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=19.46 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=18.87 +/- 0.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=18.40 +/- 0.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=18.70 +/- 0.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=18.88 +/- 0.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=16.20 +/- 5.18\n",
      "Episode length: 17.20 +/- 5.60\n",
      "Eval num_timesteps=21000, episode_reward=17.95 +/- 0.75\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=18.95 +/- 0.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=19.02 +/- 0.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=19.41 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=18.67 +/- 0.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=18.98 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=18.39 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=15.38 +/- 5.24\n",
      "Episode length: 17.00 +/- 6.00\n",
      "Eval num_timesteps=29000, episode_reward=18.89 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=19.26 +/- 0.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=18.75 +/- 0.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=19.52 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=19.33 +/- 0.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=18.43 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=19.31 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=19.28 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=18.69 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=19.02 +/- 0.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=19.32 +/- 0.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=18.85 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=18.76 +/- 0.46\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=19.12 +/- 0.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=19.06 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=18.79 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=19.34 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=19.27 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=18.57 +/- 0.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=18.84 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=18.55 +/- 0.94\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=19.27 +/- 0.59\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=18.81 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=19.28 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=53000, episode_reward=18.55 +/- 1.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=54000, episode_reward=19.12 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=16.04 +/- 6.08\n",
      "Episode length: 16.80 +/- 6.40\n",
      "Eval num_timesteps=56000, episode_reward=18.90 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=19.26 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=58000, episode_reward=19.11 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=19.42 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=18.15 +/- 0.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=61000, episode_reward=18.56 +/- 0.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=62000, episode_reward=19.37 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=63000, episode_reward=19.21 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=19.17 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=17.98 +/- 0.58\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=66000, episode_reward=18.28 +/- 0.66\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=67000, episode_reward=19.15 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=18.97 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=69000, episode_reward=18.97 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=19.01 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=71000, episode_reward=19.23 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=18.74 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=73000, episode_reward=19.06 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=74000, episode_reward=18.93 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=19.24 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=19.11 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=77000, episode_reward=19.49 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=78000, episode_reward=18.63 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=79000, episode_reward=18.99 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=19.16 +/- 0.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=81000, episode_reward=18.08 +/- 0.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=82000, episode_reward=19.18 +/- 0.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=83000, episode_reward=19.25 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=19.36 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=18.96 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=86000, episode_reward=19.26 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=87000, episode_reward=19.16 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=19.51 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=89000, episode_reward=18.63 +/- 0.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=18.59 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=91000, episode_reward=18.81 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=19.11 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=93000, episode_reward=18.93 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=94000, episode_reward=19.08 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=18.87 +/- 0.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=16.47 +/- 4.85\n",
      "Episode length: 17.40 +/- 5.20\n",
      "Eval num_timesteps=97000, episode_reward=19.02 +/- 0.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=98000, episode_reward=18.69 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=99000, episode_reward=15.79 +/- 6.90\n",
      "Episode length: 16.40 +/- 7.20\n",
      "Eval num_timesteps=100000, episode_reward=18.94 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=101000, episode_reward=18.40 +/- 0.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=102000, episode_reward=19.16 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=103000, episode_reward=18.71 +/- 0.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=18.84 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=19.00 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=106000, episode_reward=19.08 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=107000, episode_reward=19.05 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=19.41 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=109000, episode_reward=19.22 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=19.04 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=111000, episode_reward=19.17 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=19.29 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=113000, episode_reward=18.99 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=18.69 +/- 0.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=19.04 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=18.92 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=117000, episode_reward=19.21 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=118000, episode_reward=19.10 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=119000, episode_reward=19.08 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=18.80 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=121000, episode_reward=19.08 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=122000, episode_reward=18.98 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=123000, episode_reward=19.14 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=19.12 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=19.18 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=126000, episode_reward=19.11 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=127000, episode_reward=18.65 +/- 0.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=18.66 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=129000, episode_reward=19.25 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=19.06 +/- 0.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=131000, episode_reward=19.33 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=19.08 +/- 0.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=18.96 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=134000, episode_reward=19.14 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=19.33 +/- 0.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=19.00 +/- 0.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=137000, episode_reward=19.14 +/- 0.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=138000, episode_reward=19.06 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=139000, episode_reward=19.10 +/- 0.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=19.20 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=141000, episode_reward=18.77 +/- 0.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=142000, episode_reward=19.29 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=143000, episode_reward=19.27 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=19.15 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=19.05 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=146000, episode_reward=19.25 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=147000, episode_reward=19.23 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=19.21 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=149000, episode_reward=19.27 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=19.37 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=151000, episode_reward=19.34 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=19.29 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=153000, episode_reward=19.12 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=154000, episode_reward=19.03 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=19.23 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=18.78 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=157000, episode_reward=19.11 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=158000, episode_reward=19.22 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=159000, episode_reward=19.17 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=19.03 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=161000, episode_reward=19.09 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=162000, episode_reward=19.27 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=163000, episode_reward=19.27 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=18.49 +/- 0.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=19.14 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=166000, episode_reward=19.26 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=167000, episode_reward=19.21 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=18.72 +/- 0.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=169000, episode_reward=19.44 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=19.06 +/- 0.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=19.11 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=19.28 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=173000, episode_reward=19.12 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=174000, episode_reward=19.44 +/- 0.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=19.30 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=19.32 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=177000, episode_reward=19.23 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=178000, episode_reward=19.31 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=179000, episode_reward=18.74 +/- 0.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=19.38 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=181000, episode_reward=19.25 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=182000, episode_reward=18.92 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=183000, episode_reward=19.30 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=19.27 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=19.26 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=186000, episode_reward=19.10 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=187000, episode_reward=19.43 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=18.75 +/- 0.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=189000, episode_reward=19.42 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=19.19 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=191000, episode_reward=19.10 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=19.36 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=193000, episode_reward=19.19 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=194000, episode_reward=15.65 +/- 7.33\n",
      "Episode length: 16.20 +/- 7.60\n",
      "Eval num_timesteps=195000, episode_reward=19.12 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=19.42 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=197000, episode_reward=19.24 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=198000, episode_reward=19.16 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=199000, episode_reward=19.24 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=19.03 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=201000, episode_reward=19.24 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=202000, episode_reward=19.36 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=203000, episode_reward=19.21 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=19.35 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=19.38 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=206000, episode_reward=19.27 +/- 0.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=207000, episode_reward=19.22 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=19.28 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=19.29 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=19.06 +/- 0.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=211000, episode_reward=19.02 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=19.08 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=213000, episode_reward=19.09 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=214000, episode_reward=19.07 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=19.21 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=19.28 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=217000, episode_reward=19.43 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=218000, episode_reward=19.04 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=219000, episode_reward=19.34 +/- 0.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=18.71 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=221000, episode_reward=19.44 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=222000, episode_reward=19.18 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=223000, episode_reward=19.29 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=19.18 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=19.44 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=226000, episode_reward=19.33 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=227000, episode_reward=19.05 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=19.40 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=229000, episode_reward=19.35 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=19.17 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=231000, episode_reward=19.38 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=19.44 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=233000, episode_reward=19.04 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=234000, episode_reward=19.14 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=19.27 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=19.35 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=237000, episode_reward=19.27 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=238000, episode_reward=19.06 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=239000, episode_reward=19.25 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=19.19 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=241000, episode_reward=19.29 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=242000, episode_reward=19.27 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=243000, episode_reward=19.30 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=19.38 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=19.55 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=246000, episode_reward=19.17 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=19.34 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=19.04 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=249000, episode_reward=19.24 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=19.46 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=251000, episode_reward=19.38 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=19.53 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=253000, episode_reward=19.43 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=254000, episode_reward=19.29 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=19.22 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=19.17 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=257000, episode_reward=19.32 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=258000, episode_reward=19.24 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=259000, episode_reward=18.85 +/- 0.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=18.97 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=261000, episode_reward=15.54 +/- 7.27\n",
      "Episode length: 16.20 +/- 7.60\n",
      "Eval num_timesteps=262000, episode_reward=19.30 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=263000, episode_reward=18.69 +/- 0.53\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=19.19 +/- 0.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=19.35 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=19.35 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=267000, episode_reward=19.27 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=19.13 +/- 0.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=269000, episode_reward=19.19 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=19.30 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=271000, episode_reward=19.39 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=19.20 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=273000, episode_reward=19.35 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=274000, episode_reward=19.28 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=19.41 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=19.14 +/- 0.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=277000, episode_reward=19.10 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=278000, episode_reward=19.42 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=279000, episode_reward=19.19 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=19.08 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=281000, episode_reward=19.04 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=282000, episode_reward=19.09 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=283000, episode_reward=19.18 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=19.10 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=18.85 +/- 0.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=286000, episode_reward=19.19 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=287000, episode_reward=18.91 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=19.41 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=289000, episode_reward=19.30 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=19.34 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=291000, episode_reward=19.26 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=18.90 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=293000, episode_reward=18.98 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=294000, episode_reward=18.88 +/- 0.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=19.19 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=19.05 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=297000, episode_reward=19.34 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=298000, episode_reward=19.39 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=299000, episode_reward=19.36 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=19.22 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=301000, episode_reward=19.21 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=302000, episode_reward=19.34 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=303000, episode_reward=19.37 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=19.30 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=19.35 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=306000, episode_reward=19.31 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=307000, episode_reward=19.37 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=18.87 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=309000, episode_reward=19.37 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=19.41 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=311000, episode_reward=19.20 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=19.38 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=313000, episode_reward=19.16 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=314000, episode_reward=19.29 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=19.40 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=19.24 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=317000, episode_reward=19.44 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=318000, episode_reward=19.17 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=319000, episode_reward=19.28 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=19.24 +/- 0.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=321000, episode_reward=19.19 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=322000, episode_reward=19.11 +/- 0.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=18.76 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=19.46 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=19.14 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=326000, episode_reward=19.36 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=327000, episode_reward=19.22 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=19.37 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=329000, episode_reward=19.02 +/- 0.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=19.25 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=331000, episode_reward=19.41 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=19.42 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=333000, episode_reward=19.10 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=334000, episode_reward=19.18 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=19.31 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=19.43 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=337000, episode_reward=18.82 +/- 0.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=338000, episode_reward=19.26 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=339000, episode_reward=19.43 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=19.20 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=341000, episode_reward=19.43 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=19.41 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=343000, episode_reward=19.28 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=18.99 +/- 0.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=19.31 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=346000, episode_reward=19.38 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=347000, episode_reward=18.72 +/- 0.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=19.20 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=349000, episode_reward=19.53 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=19.41 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=351000, episode_reward=19.19 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=19.17 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=353000, episode_reward=19.23 +/- 0.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=354000, episode_reward=19.47 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=19.39 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=19.39 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=357000, episode_reward=19.31 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=358000, episode_reward=19.19 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=359000, episode_reward=19.23 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=18.73 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=19.29 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=362000, episode_reward=19.43 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=363000, episode_reward=19.27 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=19.33 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=19.08 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=366000, episode_reward=19.16 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=367000, episode_reward=19.22 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=18.75 +/- 0.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=369000, episode_reward=19.29 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=19.25 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=371000, episode_reward=19.12 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=19.16 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=373000, episode_reward=19.39 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=374000, episode_reward=19.37 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=19.03 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=19.25 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=377000, episode_reward=18.88 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=378000, episode_reward=19.18 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=379000, episode_reward=19.33 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=19.28 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=381000, episode_reward=17.66 +/- 2.84\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=382000, episode_reward=19.29 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=383000, episode_reward=19.31 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=19.19 +/- 0.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=19.37 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=386000, episode_reward=19.26 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=387000, episode_reward=19.30 +/- 0.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=19.12 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=389000, episode_reward=19.19 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=19.50 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=391000, episode_reward=18.79 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=19.19 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=393000, episode_reward=19.43 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=394000, episode_reward=19.27 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=19.18 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=19.29 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=397000, episode_reward=19.22 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=398000, episode_reward=19.25 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=19.06 +/- 0.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=19.07 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=401000, episode_reward=19.28 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=402000, episode_reward=19.06 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=403000, episode_reward=18.85 +/- 0.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=19.27 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=18.77 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=406000, episode_reward=18.83 +/- 0.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=407000, episode_reward=19.34 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=19.43 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=409000, episode_reward=19.31 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=19.39 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=411000, episode_reward=19.25 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=19.13 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=413000, episode_reward=19.42 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=414000, episode_reward=18.99 +/- 0.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=19.26 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=19.10 +/- 0.54\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=417000, episode_reward=19.20 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=19.41 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=419000, episode_reward=19.07 +/- 0.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=19.23 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=421000, episode_reward=19.38 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=422000, episode_reward=19.34 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=423000, episode_reward=18.92 +/- 0.38\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=19.22 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=19.27 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=426000, episode_reward=19.38 +/- 0.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=427000, episode_reward=19.18 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=19.30 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=429000, episode_reward=19.31 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=19.25 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=431000, episode_reward=19.39 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=19.24 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=433000, episode_reward=19.17 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=434000, episode_reward=19.39 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=19.32 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=19.24 +/- 0.24\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=19.24 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=438000, episode_reward=19.37 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=439000, episode_reward=19.35 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=19.33 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=441000, episode_reward=19.40 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=442000, episode_reward=19.41 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=443000, episode_reward=19.23 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=18.85 +/- 0.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=19.27 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=446000, episode_reward=19.38 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=447000, episode_reward=19.24 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=18.59 +/- 0.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=449000, episode_reward=19.10 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=19.42 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=451000, episode_reward=19.20 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=19.46 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=453000, episode_reward=19.40 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=454000, episode_reward=18.43 +/- 1.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=19.28 +/- 0.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=17.54 +/- 3.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=457000, episode_reward=17.49 +/- 2.64\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=458000, episode_reward=19.34 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=459000, episode_reward=19.32 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=19.43 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=461000, episode_reward=19.34 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=462000, episode_reward=19.20 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=463000, episode_reward=19.03 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=17.87 +/- 2.67\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=19.41 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=466000, episode_reward=19.32 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=467000, episode_reward=15.88 +/- 4.15\n",
      "Episode length: 17.60 +/- 4.80\n",
      "Eval num_timesteps=468000, episode_reward=18.56 +/- 1.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=469000, episode_reward=19.49 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=19.33 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=471000, episode_reward=19.35 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=19.21 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=473000, episode_reward=19.58 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=474000, episode_reward=19.48 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=18.76 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=19.22 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=477000, episode_reward=19.31 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=478000, episode_reward=19.46 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=479000, episode_reward=19.61 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480000, episode_reward=19.55 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=481000, episode_reward=19.46 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=482000, episode_reward=19.52 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=483000, episode_reward=19.08 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=19.44 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=19.41 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=486000, episode_reward=19.19 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=487000, episode_reward=19.58 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=18.70 +/- 0.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=489000, episode_reward=19.52 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=19.47 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=491000, episode_reward=19.31 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=19.58 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=493000, episode_reward=19.53 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=19.53 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=19.50 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=19.16 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=497000, episode_reward=19.38 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=498000, episode_reward=19.28 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=499000, episode_reward=19.53 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=19.47 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=501000, episode_reward=15.95 +/- 1.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=502000, episode_reward=19.05 +/- 0.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=503000, episode_reward=19.46 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=19.54 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=19.36 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=506000, episode_reward=19.21 +/- 0.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=507000, episode_reward=18.68 +/- 0.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=19.34 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=509000, episode_reward=18.81 +/- 0.76\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=19.21 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=511000, episode_reward=19.55 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=19.18 +/- 0.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=19.37 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=514000, episode_reward=19.43 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=19.56 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=15.68 +/- 3.78\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=517000, episode_reward=17.31 +/- 0.95\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=518000, episode_reward=19.21 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=519000, episode_reward=18.90 +/- 0.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=19.21 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=521000, episode_reward=19.55 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=522000, episode_reward=19.27 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=523000, episode_reward=19.04 +/- 0.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=19.50 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=19.46 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=526000, episode_reward=19.43 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=527000, episode_reward=19.45 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=19.58 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=529000, episode_reward=18.74 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=19.43 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=531000, episode_reward=19.37 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=19.50 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=533000, episode_reward=19.23 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=534000, episode_reward=19.30 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=19.59 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=16.53 +/- 1.52\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=537000, episode_reward=19.39 +/- 0.41\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=538000, episode_reward=16.05 +/- 3.61\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=539000, episode_reward=18.33 +/- 0.93\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=17.56 +/- 2.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=541000, episode_reward=18.85 +/- 0.79\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=542000, episode_reward=19.20 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=543000, episode_reward=19.45 +/- 0.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=19.44 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=19.21 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=546000, episode_reward=19.42 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=547000, episode_reward=19.39 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=19.15 +/- 0.29\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=549000, episode_reward=18.62 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=19.25 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=19.21 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=19.26 +/- 0.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=553000, episode_reward=15.94 +/- 4.56\n",
      "Episode length: 17.40 +/- 5.20\n",
      "Eval num_timesteps=554000, episode_reward=13.13 +/- 6.65\n",
      "Episode length: 16.20 +/- 7.60\n",
      "Eval num_timesteps=555000, episode_reward=18.53 +/- 0.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=18.85 +/- 0.60\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=557000, episode_reward=18.99 +/- 0.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=558000, episode_reward=15.11 +/- 3.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=559000, episode_reward=16.04 +/- 1.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=16.99 +/- 1.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=561000, episode_reward=18.14 +/- 1.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=562000, episode_reward=18.40 +/- 1.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=563000, episode_reward=16.33 +/- 3.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=18.59 +/- 0.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=18.44 +/- 0.55\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=566000, episode_reward=18.94 +/- 0.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=567000, episode_reward=18.21 +/- 0.82\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=18.58 +/- 0.57\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=569000, episode_reward=18.27 +/- 2.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=17.92 +/- 1.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=571000, episode_reward=18.31 +/- 1.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=18.62 +/- 0.56\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=573000, episode_reward=17.82 +/- 0.91\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=574000, episode_reward=19.20 +/- 0.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=18.45 +/- 0.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=18.41 +/- 0.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=577000, episode_reward=16.88 +/- 1.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=578000, episode_reward=18.38 +/- 0.70\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=579000, episode_reward=19.00 +/- 0.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=18.71 +/- 0.49\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=581000, episode_reward=17.62 +/- 2.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=582000, episode_reward=17.07 +/- 2.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=583000, episode_reward=19.10 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=16.87 +/- 1.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=16.64 +/- 3.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=586000, episode_reward=17.51 +/- 1.39\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=587000, episode_reward=15.72 +/- 2.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=16.78 +/- 1.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=18.68 +/- 0.62\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=17.09 +/- 2.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=591000, episode_reward=18.02 +/- 0.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=16.49 +/- 1.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=593000, episode_reward=16.83 +/- 2.71\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=594000, episode_reward=18.47 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=18.54 +/- 0.77\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=18.29 +/- 0.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=597000, episode_reward=17.95 +/- 1.11\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=598000, episode_reward=15.41 +/- 2.90\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=599000, episode_reward=18.44 +/- 0.98\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=18.39 +/- 0.48\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=601000, episode_reward=18.41 +/- 0.74\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=602000, episode_reward=19.23 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=603000, episode_reward=18.12 +/- 1.85\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=19.24 +/- 0.26\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=18.79 +/- 0.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=606000, episode_reward=18.98 +/- 0.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=607000, episode_reward=19.11 +/- 0.40\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=19.23 +/- 0.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=609000, episode_reward=18.70 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=19.61 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=611000, episode_reward=19.05 +/- 0.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=19.25 +/- 0.47\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=613000, episode_reward=19.42 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=614000, episode_reward=19.34 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=19.32 +/- 0.32\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=18.60 +/- 0.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=617000, episode_reward=19.07 +/- 0.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=618000, episode_reward=19.54 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=619000, episode_reward=19.44 +/- 0.27\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=19.31 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=621000, episode_reward=19.19 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=622000, episode_reward=19.32 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=623000, episode_reward=16.14 +/- 6.59\n",
      "Episode length: 16.60 +/- 6.80\n",
      "Eval num_timesteps=624000, episode_reward=19.38 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=19.39 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=626000, episode_reward=19.27 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=19.57 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=19.52 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=629000, episode_reward=19.31 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=19.35 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=631000, episode_reward=15.62 +/- 7.31\n",
      "Episode length: 16.20 +/- 7.60\n",
      "Eval num_timesteps=632000, episode_reward=19.36 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=633000, episode_reward=19.44 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=634000, episode_reward=19.32 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=19.31 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=636000, episode_reward=19.23 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=637000, episode_reward=19.54 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=638000, episode_reward=19.39 +/- 0.21\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=639000, episode_reward=19.35 +/- 0.16\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=19.26 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=641000, episode_reward=19.17 +/- 0.25\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=642000, episode_reward=19.29 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=643000, episode_reward=19.33 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=644000, episode_reward=19.34 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=19.09 +/- 0.31\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=19.33 +/- 0.18\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=647000, episode_reward=19.24 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=19.37 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=649000, episode_reward=19.36 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=19.43 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=651000, episode_reward=19.39 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=652000, episode_reward=19.16 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=653000, episode_reward=19.10 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=654000, episode_reward=19.18 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=19.22 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=19.29 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=657000, episode_reward=19.39 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=658000, episode_reward=19.03 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=659000, episode_reward=19.30 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=19.41 +/- 0.13\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=661000, episode_reward=19.00 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=662000, episode_reward=18.77 +/- 0.45\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=663000, episode_reward=19.24 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=18.92 +/- 0.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=17.63 +/- 1.30\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=666000, episode_reward=18.46 +/- 0.42\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=667000, episode_reward=18.36 +/- 0.43\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=18.55 +/- 0.44\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=669000, episode_reward=17.36 +/- 1.50\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=16.75 +/- 1.63\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=671000, episode_reward=18.40 +/- 0.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=18.36 +/- 0.28\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=673000, episode_reward=18.24 +/- 0.97\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=674000, episode_reward=18.96 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=18.89 +/- 0.23\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=18.97 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=677000, episode_reward=19.07 +/- 0.36\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=678000, episode_reward=18.35 +/- 0.87\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=679000, episode_reward=19.06 +/- 0.15\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=18.97 +/- 0.33\n",
      "Episode length: 20.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "eval_callback = EvalCallback(eval_env,  eval_freq = 1000 ,deterministic = True )\n",
    "model.learn(total_timesteps= 1000000 , callback = eval_callback )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAC_path = os.path.join('Training', 'Saved Models', 'ElasticadeskGPU1_SAC')   # Keep all the files in same directory\n",
    "os.makedirs(os.path.dirname(SAC_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(SAC_path)\n",
    "# print(f\"Model saved at: {SAC_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: Training\\Saved Models\\ElasticadeskGPU1_SAC\n"
     ]
    }
   ],
   "source": [
    "model=SAC.load(SAC_path)\n",
    "print(f\"Model loaded from: {SAC_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window gained input focus\n",
      "20\n",
      "[0.8892140579441836, 0.9543835886808281, 0.961565895983912, 0.9612192572563479, 0.9611199714996557, 0.9613644263474418, 0.9615340143645049, 0.9616439695686239, 0.9617157443529699, 0.9617634598881405, 0.9617949227347149, 0.9618161811583698, 0.9618302607546404, 0.9618400604273442, 0.9618463333619961, 0.961850802280918, 0.9618537986112494, 0.9618557655950593, 0.961856979180441, 0.9618580649226268]\n",
      "Episode:1 Score:19.153927554913963\n",
      "20\n",
      "[0.982400707783382, 0.963628145498634, 0.9635373964860683, 0.9634507603575662, 0.9633775580668753, 0.9633159677886763, 0.96326383588384, 0.9632191224599704, 0.9631806523219998, 0.9631474689619339, 0.9631187062147025, 0.9630936531835493, 0.9630717103138116, 0.9630529337435195, 0.9630362438642563, 0.9630216206911368, 0.9630090402371986, 0.9629979421808642, 0.962988101047379, 0.9629798540559237]\n",
      "Episode:2 Score:19.282891421141287\n",
      "20\n",
      "[0.926488118437212, 0.9546635541571887, 0.9659520631084862, 0.967859679234739, 0.9680274712563756, 0.9680601784823346, 0.9680779743485404, 0.9680907919006149, 0.9681004279038027, 0.9681077884632235, 0.9681134596387149, 0.9681179617222588, 0.9681214003229449, 0.9681240541406112, 0.9681262369360653, 0.968127900184544, 0.9681291996257659, 0.9681302979096122, 0.9681312508895894, 0.9681317842475216]\n",
      "Episode:3 Score:19.304681592910146\n",
      "20\n",
      "[0.9498703871405295, 0.9648872198525565, 0.967581051291913, 0.9681143326712348, 0.9683093279913972, 0.9683215352149689, 0.9682208948588295, 0.9680539537593729, 0.9682380912673615, 0.9676953181604632, 0.9674518202107717, 0.9672520346292128, 0.9670735737096066, 0.9669153672760568, 0.9667774106501232, 0.9666588559635181, 0.9665583508189511, 0.9664735396824428, 0.9664025897235303, 0.96634382026957]\n",
      "Episode:4 Score:19.32719947514241\n",
      "20\n",
      "[0.9272940005173211, 0.9561437967105809, 0.9662597770581841, 0.9672162482445475, 0.9672966690013983, 0.967314395480652, 0.9673251720614994, 0.9673333503790228, 0.9673394746854999, 0.9673443614771933, 0.9673482823991489, 0.9673515447084133, 0.9673538116475926, 0.9673558543067048, 0.9673574560791128, 0.9673587072409557, 0.967359602431235, 0.9673603751528054, 0.9673610404789384, 0.9673615962719505]\n",
      "Episode:5 Score:19.294435516332754\n",
      "20\n",
      "[0.8847326066302739, 0.9515236858003859, 0.9616271309532612, 0.9638924788585344, 0.96403204733031, 0.9640338307440521, 0.9640497933157343, 0.9640615384030715, 0.9640680236129817, 0.9640715176337366, 0.9640732310927519, 0.9640740773523453, 0.9640744287739316, 0.9640747396138741, 0.9640747608310355, 0.9640747698982638, 0.9640747729184936, 0.9640747759354628, 0.9640747759354628, 0.9640747759354628]\n",
      "Episode:6 Score:19.18683776156942\n",
      "20\n",
      "[0.9293781868125882, 0.9654190356676594, 0.9840482319635642, 0.9770126098291001, 0.9695421400725831, 0.9622984486421506, 0.9521409742830499, 0.9427713110571841, 0.9376940232072335, 0.9366169282820115, 0.9364183289671784, 0.9363885839189129, 0.9363688586894566, 0.9363542815526764, 0.9363427832468332, 0.9363335834339941, 0.9363263723665086, 0.9363209122806045, 0.9363164121282371, 0.9363129624645987]\n",
      "Episode:7 Score:18.920404968866126\n",
      "20\n",
      "[0.9398334567218757, 0.9484644745198522, 0.954927664169111, 0.9595295176687431, 0.9624117558517618, 0.9637706499075974, 0.9634614620893174, 0.9625553704453327, 0.9617415632533102, 0.9612074992624963, 0.9609105457090555, 0.9607653429043994, 0.9607052925734214, 0.9606880295899224, 0.9606895101473554, 0.9606975443713671, 0.9607059848238849, 0.9607129630675344, 0.9607178163562188, 0.9607208551663676]\n",
      "Episode:8 Score:19.18521729859893\n",
      "20\n",
      "[0.9797850554482298, 0.9617109045972608, 0.9626670851310357, 0.9626558930786324, 0.9624835687632912, 0.9622767630184788, 0.9620568939651019, 0.9618273154404073, 0.9615878418236364, 0.9613366107368564, 0.9610716759061899, 0.9607906078233941, 0.9604904834905132, 0.9601672231540406, 0.9598166265858652, 0.9594325181415708, 0.9590079730111052, 0.9585337436014614, 0.9579982582747963, 0.9573860235554749]\n",
      "Episode:9 Score:19.23308306554734\n",
      "20\n",
      "[0.9673387659489291, 0.9682959397547832, 0.9669220642311782, 0.9652853810361295, 0.9631749333710223, 0.962784581279255, 0.9631927150868924, 0.963897822289317, 0.9646266416063781, 0.965224190474889, 0.965627047818902, 0.9658430725367995, 0.9659214169208282, 0.9659210233656605, 0.9658897172985774, 0.9658558440924611, 0.9658310206730498, 0.9658174886985746, 0.9658124014237695, 0.9658119411105616]\n",
      "Episode:10 Score:19.30907400901796\n"
     ]
    }
   ],
   "source": [
    "env.enable_render = True\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state  , info = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    truncation=False\n",
    "    test=[]\n",
    "    n_score=[]\n",
    "    dis = []\n",
    "    while not (done or truncation):\n",
    "        action,_ = model.predict(state , deterministic=True)\n",
    "        #print(action)\n",
    "        state, reward, done, truncation ,info = env.step(action)\n",
    "        n_score.append(reward)\n",
    "        dis.append(state[-1])\n",
    "        score+=reward\n",
    "        test.append(done)\n",
    "    #print(test)\n",
    "    print(len(test))\n",
    "    print(n_score)\n",
    "    #print(np.min(dis))\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
